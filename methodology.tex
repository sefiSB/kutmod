\section{Methodology} \label{sec:methodology}
This section describes the empirical methodology used to evaluate the Automated Program Repair (APR) system, focusing specifically on concurrent programs. The evaluation aims to measure the system's effectiveness (patch accuracy), robustness (side effects), and efficiency (time of reply).
\subsection{Experimental Setup and Dataset} \label{subsec:dataset}
The challenge in repairing concurrent programs stems from their inherent non-determinism ~\cite{yu2018,chang2021}
 and the complexity arising from the large number of possible thread interleavings ~\cite{abbaspour2016}
.
\begin{itemize} \item \textbf{Program Corpus (Concurrent Programs):} The evaluation utilizes a custom dataset consisting of 43 buggy concurrent programs. This dataset is designed to assess the repair capability across varying degrees of complexity and bug density ~\cite{yu2018}
: \begin{enumerate} \item 6 programs containing 1 concurrency defect (e.g., data race, deadlock) ~\cite{jain2025,liu2021_2}. \item 10 programs containing 2 concurrency defects. \item 11 programs containing 3 concurrency defects. \item 10 programs containing 4 concurrency defects.\item 6 programs containing 5 concurrency defects. \end{enumerate} \item \textbf{Testing Oracle:} Each program in the corpus is accompanied by a suite of exhaustive tests. Exhaustive testing is used as the oracle to validate the correctness of the generated patches. Common concurrency benchmarks mentioned in related research include the SV-COMP  tests and ARM Litmus tests ~\cite{yang2023,jain2025}, and SCTBench (often used for C/C++ concurrency testing) ~\cite{anand2024}. \item \textbf{Patch Validation Goal:} The goal of the validation process is to determine how many original bugs the patch successfully fixes, and critically, to determine if the resulting program fails elsewhere or introduces new instability ~\cite{liyanage2025}
. \end{itemize}
\subsection{Evaluation Metrics} \label{subsec:metrics}
APR tool evaluation should transcend merely reporting the number of plausible patches found (the "plausible patch problem") and focus on measuring correct patches and efficiency ~\cite{yang2023,liu2021_1}
. We evaluate the system using three primary metrics derived from the requirements.
\subsubsection{Patch Accuracy} This metric quantifies the effectiveness of the APR system by measuring the ratio of correctly fixed bugs to the total number of bugs presented in the dataset ~\cite{yang2025,liu2021_1}
.
\begin{itemize} \item \textbf{Correct Patch Definition:} A patch is considered correct if it resolves the original defect(s) and maintains the original program semantics ~\cite{liu2021_2}
. In research, correctness is generally assessed by comparing the generated patch against the developer-provided patch (if available) or by ensuring the patched program passes an exhaustive suite of tests without breaking necessary behavior ~\cite{yang2023,liu2021_1}. \item \textbf{Fixed Bugs Count:} This evaluation checks the extent to which the tool successfully fixed the 1, 3, or 5 concurrency defects present in the targeted programs ~\cite{liu2021_2}
. \end{itemize}
\subsubsection{Introducing New Bugs (Side Effects and Robustness)} This crucial metric measures the robustness of the generated patch by identifying unintended negative consequences, known as side effects or new violations ~\cite{liyanage2025}
.
\begin{itemize} \item \textbf{Regression Failures:} We assess whether the repaired code causes unit tests that previously passed on the original buggy code to fail (a common check for semantic and functional preservation)
. Failures might include compilation errors (e.g., "cannot find symbol") or run-time errors (e.g., IllegalAccessError or assertion errors) ~\cite{liyanage2025}. \item \textbf{New Violations/Code Quality:} The generated patch is analyzed to detect if it introduces new bugs or code smells ~\cite{liyanage2025}. This is a major concern, as some studies found tools introducing thousands of new faults (e.g., Sorald introduced 2,120 new faults, showing the utility of this robust check) ~\cite{liyanage2025}. Tools like Fix2Fit and CPR emphasize checking the "fit" in the whole program context to decrease side effects that might result in new bugs ~\cite{anand2024}
. \end{itemize}
\subsubsection{Time of Reply (Efficiency)} The time of reply, or repair efficiency, is measured as the computational time cost associated with generating a viable patch ~\cite{liu2021_1}
.
\begin{itemize} \item \textbf{Measurement:} Efficiency is assessed by quantifying the effort to yield a plausible/correct patch ~\cite{liu2021_1}
. The measurement typically covers the time elapsed from initiating the repair process until the generation and validation of the patch candidate ~\cite{liu2021_1}. \item \textbf{Relevance:} High patch generation expense in terms of time may deter adoption in practical development environments, thus efficiency measurements are highly important ~\cite{liu2021_1}
. \end{itemize}
\subsection{Patch Validation Protocol}
Due to the nature of concurrency bugs, standard test suite validation often results in overfitting patches or false positives, where the patch passes available tests but does not correctly fix the underlying issue ~\cite{yang2023,liu2021_1}
.
\begin{itemize} \item \textbf{Concurrency Validation:} The use of exhaustive tests is critical, as tests for concurrent software are notoriously difficult to create ~\cite{abbaspour2016}
. The system must ensure that the fix reliably eliminates non-deterministic faulty interleavings, rather than merely masking the bug ~\cite{liu2021_1}. \item \textbf{Root Cause Confirmation:} For complex bugs, such as concurrency errors, validation involves checking if the fix introduces unintended synchronizations that cause problems (e.g., deadlocks) ~\cite{liu2021_2,deng2015f}. Techniques sometimes require iteratively adding inter-thread ordering constraints to eliminate erroneous schedules to compute repairs ~\cite{khoshnood2015}. \end{itemize}