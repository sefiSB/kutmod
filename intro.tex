\section{Introduction}

\subsection{Automated Program Repair (APR) and the LLM Revolution}

Automated Program Repair (APR) fundamentally represents a transformative approach in software engineering, with the core objective of reducing the time and resources required to identify and fix bugs in code by leveraging computational techniques to streamline debugging and maintenance~\cite{dikici2025}. APR methodologies have evolved significantly, moving from earlier template-based and classical machine learning (ML) approaches to deep learning-based techniques~\cite{dikici2025}. Deep learning pushed the boundaries by applying sophisticated models capable of analyzing the semantic context of code, which enables the generation of more accurate and generalizable fixes~\cite{dikici2025}.

The field is now undergoing a radical shift due to the explosive growth of Large Language Models (LLMs)~\cite{yang2025}. LLMs like OpenAI’s Codex, CodeLlama, and DeepSeek-Coder are actively reshaping automated software engineering ~\cite{yang2025}~\cite{anand2024}. These code-focused LLMs possess the inherent advantage of being trained on massive datasets with billions of parameters, enabling impressive performance in tasks such as code generation and bug fixing, often surpassing models trained from scratch ~\cite{anand2024}. LLM-based APR (LLM-APR) systems demonstrate significant potential to understand and generate code, allowing them to fix program bugs with minimal human intervention ~\cite{yang2025}.

\subsection{The Critical Challenge of Concurrent Programming}

The drive for maximizing performance and optimizing resource utilization, necessitated by the increasing complexity of modern software systems and the prevalence of multi-core processors, has made concurrent programming essential~\cite{jain2025}. However, concurrency introduces critical software challenges~\cite{jain2025}. Parallel programming is notorious for its non-deterministic nature, which makes verifying and testing concurrent programs one of the most difficult tasks in software development~\cite{jain2025}~\cite{chang2021}. Key concurrency problems, such as deadlocks, synchronization errors, and race conditions, can lead to unpredictable behavior and critical failures~\cite{jain2025}. A detailed study of fixed bug reports in the Apache Hadoop project revealed that concurrency bugs accounted for only a small percentage of total reported bugs (6.15\%), but data races constituted the largest portion of bugs categorized with the highest severity (“Blocker”)~\cite{abbaspour2016}.

LLMs offer a promising capability to assist in this complex domain, demonstrating natural language reasoning and the ability to detect basic concurrency issues in simpler programs~\cite{jain2025}. Highly capable models like GPT-4 exhibit a robust understanding of issues such as deadlocks and data races when evaluated under a basic sequentially consistent memory model~\cite{jain2025}. Despite this, all evaluated LLMs struggle significantly to verify program correctness when dealing with the intricacies of relaxed memory models (RMM) like Total Store Order (TSO) or Partial Store Order (PSO), often failing to accurately capture complex memory ordering constraints or consistently produce feasible error traces leading to failures~\cite{jain2025}.

\subsection{Mapping the LLM-APR Landscape and Evaluation Challenges}

In navigating this rapidly evolving field, a comprehensive survey identifies four major LLM-APR paradigms: fine-tuning, prompting, procedural pipelines, and agentic frameworks~\cite{yang2025}. These paradigms reveal key trade-offs: fine-tuning achieves strong task alignment but demands significant computational investment and large datasets~\cite{yang2025}, while prompting enables rapid deployment but is inherently limited by prompt design and the LLM's fixed context window~\cite{yang2025}. Furthermore, these approaches are often enhanced by Retrieval-Augmented Generation (RAG), which injects external knowledge, and Analysis-Augmented Generation (AAG), which incorporates program analysis results (e.g., dynamic error traces or static diagnostics)~\cite{yang2025}.

A critical focus is placed on the need for comprehensive evaluation methodologies~\cite{liyanage2025}. Evaluating APR tools cannot solely focus on their bug-fixing capabilities, as this neglects potential negative consequences~\cite{liyanage2025}. Key persistent challenges include:

\begin{enumerate}
    \item \textbf{Verifying Semantic Correctness}: Ensuring that a patch maintains the intended code functionality, particularly beyond the limited coverage provided by test suites~\cite{yang2025}.
    \item \textbf{Mitigating Side Effects and New Errors}: Preventing the automated introduction of new bugs, security vulnerabilities, or code violations during the repair process~\cite{yang2025}~\cite{liyanage2025}. For example, studies on the Java repair tool Sorald showed that while it achieved a high fix rate for targeted issues, it simultaneously introduced a large number of new violations, including several bugs and numerous code smells, highlighting a critical absence of semantic and stylistic understanding in the repair mechanism~\cite{liyanage2025}.
    \item \textbf{Preventing Code Structure Degradation}: Automated repairs often negatively affect code quality by increasing metrics related to complexity and reducing cohesion (e.g., increasing LCOM1, WMC, and Lines of Code), which makes the code harder to understand and maintain~\cite{liyanage2025}. However this is outside the scope of this study.
\end{enumerate}

Addressing these challenges is crucial for transitioning LLM-based APR from prototypes to reliable and efficient components of continuous integration and development practices~\cite{yang2025}.

\subsection{Scope of This Study}
This study aims to provide a systematic and rigorous empirical evaluation of the APR capabilities of available LLMs (ChatGPT, Claude, Gemini, DeepSeek and Copilot) when confronted with concurrent programming problems.
To thoroughly assess the performance of these models, the scope of this study focuses on three critical evaluation dimensions: 
\begin{enumerate}
    \item Patch Accuracy (Correctness)
    \item Newly Introduced Bugs (Side Effects)
    \item Time of Reply (Efficiency) 
\end{enumerate}
The experimental design utilizes a structured dataset of concurrent code snippets featuring varying levels of complexity, specifically containing 1, 3, and 5 bugs. This approach ensures the ability to compare performance on simple versus complex (multi-hunk) defects. The entire set of tests will be executed with all available LLMs to provide a direct and robust comparative analysis of their specialized capabilities in addressing concurrency-related software defects.
