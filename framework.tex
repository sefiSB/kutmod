\section{Framework: The Design Space of LLM-based APR and Critical Evaluation}
\label{sec:framework}

The theoretical framework governing modern automated software development centers on the integration of Large Language Models (LLMs) into two core research areas: Automated Program Repair (APR) and code generation ~\cite{anand2024}. Crucially, this framework must also incorporate mechanisms for assessing the quality and semantic correctness of AI-generated repairs, particularly when dealing with complex issues like concurrency bugs ~\cite{anand2024}
.
\subsection*{The Evolution of Automated Program Repair (APR) and LLMs}
Automated Program Repair (APR) and code generation have been fundamental topics in software development, aiming to reduce the time and resources required for identifying and fixing code defects ~\cite{anand2024,dikici2025}
. The emergence of Large Language Models (LLMs) has profoundly reshaped these domains, providing powerful new tools for both tasks ~\cite{anand2024,yang2025}
.
LLMs, often pre-trained on massive datasets with billions of parameters, exhibit capabilities such as understanding code structure and generating code based on natural language requests ~\cite{anand2024}
. This has led to significant improvements in the quality and speed of automating programming tasks, including bug fixing in pre-existing code and understanding complex repositories ~\cite{anand2024}
.
Traditional APR techniques relied on methods such as Abstract Syntax Trees (ASTs), heuristics for ranking patches, patterns, and context-matching ~\cite{anand2024}
. These have evolved into learning-based APR methods, which include sophisticated models like CodeT5, GraphCodeBERT, and Codex ~\cite{anand2024,yang2023}. Modern research has divided APR techniques broadly into four categories: Heuristic-based, Template-based, Constraint-based, and Learning-based, with LLMs representing the latest advancement in the latter category ~\cite{yang2023}
.
\subsection*{Taxonomy of LLM-based Automated Program Repair (LLM-APR)}
LLM-based APR (LLM-APR) systems are categorized into four primary design paradigms, distinguished by how they manage the interplay between model parameters and the control flow of the repair process ~\cite{yang2025}
:
\begin{enumerate} \item \textbf{Fine-Tuning (FT):} This approach adapts a pre-trained LLM by updating its weights using bug-fix data ~\cite{yang2025}
. Fine-tuning (FT) strategies include Parameter-Efficient Fine-Tuning (PEFT), Reinforcement Learning Fine-Tuning (RLFT), and Knowledge Distillation ~\cite{yang2025}. FT typically delivers strong task alignment and high accuracy but demands significant GPU resources ~\cite{yang2025}. \item \textbf{Prompting:} The LLM operates as a frozen component, generating a complete patch after a single query ~\cite{yang2025}. This paradigm enables rapid deployment but its efficiency is heavily dependent on the quality of the prompt design and the limits of the context window ~\cite{yang2025}. Prompting encompasses zero-shot and few-shot methods ~\cite{yang2025}. \item \textbf{Procedural Pipelines:} This involves embedding LLM inferences within fixed, scripted, multi-step workflows ~\cite{yang2025}. This structure ensures reproducible outcomes and includes approaches like Test-in-the-Loop, which alternates generation with unit tests, and Human-in-the-Loop pipelines, which incorporate developer input or hints ~\cite{yang2025}. \item \textbf{Agentic Frameworks:} In this highly autonomous paradigm, the language model determines the next repair step (e.g., planning, tool invocation, termination) rather than following a fixed script ~\cite{yang2025}. Agentic frameworks, such as Tool-Augmented Agents and LLM-as-Judges, are capable of tackling complex, multi-hunk, or cross-file bugs, though at the cost of increased latency and complexity ~\cite{yang2025}
. \end{enumerate}
Furthermore, all paradigms can be enhanced by two crucial augmentation layers ~\cite{yang2025}
: \begin{itemize} \item \textbf{Retrieval-Augmented Generation (RAG):} Augmenting the LLM's input with external knowledge, such as relevant code snippets, documentation, or historical fixes, to ground the patch in concrete domain information ~\cite{yang2025}. \item \textbf{Analysis-Augmented Generation (AAG):} Incorporating results from program analysis (static or dynamic), such as error traces, failing test logs, or data-flow facts, to guide the model toward the root cause ~\cite{yang2025}
. \end{itemize}
\subsection*{Defect Classification and Quality Assessment in AI Systems}
The quality assessment of software repairs, especially those performed by AI systems, necessitates specialized frameworks because conventional defect analysis models fail to capture the unique attributes of AI, such as data dependencies, adaptive learning processes, and decision-making logic ~\cite{alannsary2025}
.
The proposed AIODC framework, an adaptation of the Orthogonal Defect Classification (ODC) paradigm, addresses this gap by introducing AI-specific classification dimensions: Data, Learning, and Thinking ~\cite{alannsary2025}
. \begin{itemize} \item Defects occurring during the Learning phase are frequently the most prevalent and are significantly linked to high-severity classifications ~\cite{alannsary2025}. \item Defects identified in the Thinking phase disproportionately affect the system's trustworthiness and accuracy ~\cite{alannsary2025}
. \end{itemize}
Beyond classifying the origin of the defect, a comprehensive evaluation framework for APR tools must critically assess the repair outcome by scrutinizing three main areas, as demonstrated by studies like the one conducted on Sorald ~\cite{liyanage2025}
: \begin{enumerate} \item \textbf{Introduction of New Errors:} Assessing the extent to which automated repairs introduce unintended side effects, such as new bugs or code smells ~\cite{liyanage2025}. \item \textbf{Preservation of Functionality:} Verifying that the repair preserves the original code semantics and functional behavior, typically by running pre-existing unit tests on the patched code ~\cite{liyanage2025}. \item \textbf{Degradation of Code Structure:} Evaluating the impact of the repair on code quality metrics (e.g., complexity, cohesion) using metrics like LOC, WMC, and LCOM1 ~\cite{liyanage2025}. Repairs that increase LCOM1, LOC, and WMC are generally found to degrade code structure ~\cite{liyanage2025}
. \end{enumerate}
\subsection*{The Challenge of Concurrency Bugs and Verification}
A major challenge in software engineering, intensified by the prevalence of multi-core hardware, is the verification and repair of concurrency issues, such as data races, deadlocks, and synchronization errors ~\cite{jain2025,abbaspour2016}
. Concurrent programming is inherently non-deterministic, making testing and debugging notoriously difficult ~\cite{jain2025,chang2021,abbaspour2016}
.
The verification of concurrent programs is further complicated by the use of Relaxed Memory Models (RMMs), such as Total Store Order (TSO) and Partial Store Order (PSO), which define how memory operations are ordered and observed across different threads in modern architectures (e.g., ARM, x86) ~\cite{jain2025}
.
LLMs are being evaluated for their capacity in this domain by assessing their ability to: \begin{itemize} \item Understand and summarize concurrent programs ~\cite{jain2025}
. \item Identify concurrency issues like data races and deadlocks under the Sequentially Consistent (SC) memory model ~\cite{jain2025}. \item Predict correct program behaviors and verify correctness conditions across both SC and RMMs (TSO and PSO) ~\cite{jain2025}. \end{itemize} While LLMs, especially GPT-4, demonstrate natural language reasoning strengths, they still perform weaker than dedicated verification tools in handling RMMs and often fail to accurately explain the causal relationships of complex concurrency failures ~\cite{jain2025}. Dedicated concurrency benchmarks, such as SV-COMP's \texttt{pthread} tests and ARM Litmus tests, are necessary to evaluate these capabilities rigorously ~\cite{jain2025}.