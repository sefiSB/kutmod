\section{Framework: The Design Space of LLM-based APR and Critical Evaluation}
\label{sec:framework}

This section outlines the conceptual framework and classification systems used to analyze the field of Automated Program Repair (APR), particularly focusing on Large Language Model (LLM) driven paradigms (\emph{LLM-APR}) [1]. This framework serves a dual purpose: first, classifying the latest LLM-APR techniques based on their design choices, and second, emphasizing the critical need for a \textbf{comprehensive evaluation} of APR systems that extends beyond simple fix rates to detect harmful side effects [2, 3].

\subsection{Taxonomy of LLM-based APR Methodologies}
\label{ssec:taxonomy}

Code-focused LLMs, such as OpenAI's Codex, CodeLlama, and DeepSeek-Coder, have demonstrated the ability to understand and generate code, facilitating bug fixing with minimal human intervention [1]. The current landscape of LLM-APR, spanning January 2022 to June 2025, covers 63 distinct systems [4]. These methods are classified based on the dimensions of model \emph{parameters} (whether they are updated) and \emph{control flow} (who dictates the subsequent step) [4].

\begin{enumerate}
    \item \textbf{Fine-Tuning (FT) Approaches}:
    These methods update the weights of pre-trained LLMs using datasets of bug-fix pairs, resulting in strong task-specific adaptation and higher accuracy [5, 6]. Variants include Full Fine-Tuning (e.g., VulMaster, RepairCAT) [4, 7], Parameter-Efficient Fine-Tuning (PEFT, e.g., RepairLLaMA, MORepair) [4, 7, 8], Knowledge Distilling (e.g., KNOD, DistiLRR) [4, 9, 10], and Reinforcement Learning Fine-Tuning (RLFT) [4, 10]. A drawback of full FT is its heavy GPU demand and frequent overfitting to the suspected bug location [6].

    \item \textbf{Prompting Approaches}:
    The pre-trained model remains \textbf{frozen} and generates a patch in a single turn [11]. Deployment is rapid as it requires no additional training cost [11].
    \begin{itemize}
        \item \emph{Zero-Shot Prompting}: Provides only the buggy code and a terse instruction (e.g., "fix the bug") [11]. Early studies showed that zero-shot LLMs (like Codex) could outperform state-of-the-art APR techniques, but performance is sensitive to subtle prompt edits [11, 12].
        \item \emph{Few-Shot Prompting}: Includes a small set of bug-fix exemplars before the target defect to infer an edit pattern, often stabilizing generation and improving accuracy [13]. The success of this approach relies on careful curation or automated retrieval of highly similar examples [14, 15].
    \end{itemize}

    \item \textbf{Procedural Pipelines}:
    LLM calls are embedded into fixed, multi-step workflows to ensure reproducible output [4]. Examples include iterative testing loops (\emph{Test-in-the-Loop}, e.g., ChatRepair, ThinkRepair) [4, 16] and human intervention protocols (\emph{Human-in-the-Loop}, e.g., CREF) [4].

    \item \textbf{Agentic Frameworks}:
    The LLM operates as an autonomous agent that \textbf{decides the next step} and invokes external tools (such as testing, compilation, or static analysis) [4, 17].
    \begin{itemize}
        \item \emph{Tool-Augmented Agents} (e.g., SWE-Agent, RepairAgent, LANTERN): These agents improve over one-shot prompting by allowing the LLM to choose external actions [17, 18].
        \item \emph{LLM-as-Judges} (e.g., VulSifter, LLM4PatchCorrect): Models are used as critics to rank or refine patch candidates based on various criteria [4, 18, 19].
    \end{itemize}
\end{enumerate}

\subsubsection*{Cross-Cutting Augmentation Layers}
The efficacy of these paradigms is frequently enhanced by augmenting the context provided to the LLM:
\begin{itemize}
    \item \textbf{RAG} (\emph{Retrieval-Augmented Generation}): Incorporates external, high-signal structural context, such as relevant code snippets fetched from a repository using a Knowledge Graph (e.g., KGCompass) or dynamically collected failing assertions (e.g., D4C) [20-22].
    \item \textbf{AAG} (\emph{Analysis-Augmented Generation}): Integrates results from program analysis tools, such as semantic scoping (Appatch) or static analysis logs, to guide the model's output [21].
\end{itemize}

\subsection{A Critical Framework for APR System Evaluation}
\label{ssec:evaluation}

The evolution of APR evaluation has shifted from measuring the plausibility of generated patches to accurately counting \textbf{correct patches}, primarily due to the widespread issue of \textbf{overfitting} to test suites [2, 23, 24]. To rigorously validate LLM-APR tools, a comprehensive evaluation framework must analyze three critical factors beyond simple test passing:

\begin{enumerate}
    \item \textbf{New Violations and Code Smells}:
    It is crucial to assess whether the repair process introduces new coding defects or code smells [25, 26]. The Sorald case study demonstrated that while fixing SonarQube issues, the tool introduced \textbf{2120 new violations} (including 32 bugs and 2088 code smells) [26, 27].

    \item \textbf{Functional Correctness and Semantic Preservation}:
    APR evaluation must verify that the generated patch maintains the original program's intended behavior [28]. This is typically validated by rerunning the original \textbf{unit tests} on the patched code [3].

    \item \textbf{Code Structure Degradation}:
    The impact of the repair on code quality and maintainability must be measured [25]. Metrics from the Chidamber and Kemerer (CK) metrics suite, such as WMC (Weighted Methods per Class), DIT (Depth of Inheritance Tree), CBO (Coupling Between Objects), LCOM1 (Lack of Cohesion of Methods), and LOC (Lines of Code), are used for this analysis [25, 29-31]. Statistical analysis (e.g., Wilcoxon signed rank test) is necessary to determine if metrics significantly change post-repair [25, 29]. For instance, Sorald repairs were found to increase metrics like LCOM1, LOC, and WMC, indicating a degradation of code quality [29].
\end{enumerate}

\subsection{Special Challenge: Concurrency Bug Analysis}
\label{ssec:concurrency}

Concurrency remains a significant challenge for automated tools, including LLMs, due to the inherent \textbf{non-determinism} and the subtle interactions between threads, which lead to an "astronomical number of interleavings" [32-35].

\begin{enumerate}
    \item \textbf{Nature of Concurrency Bugs}:
    \textbf{Data races} (race condition vulnerabilities) are the most prevalent concurrency bugs, accounting for a staggering **80\%** of all such defects [36]. \textbf{Deadlocks} are also critical issues in complex environments, particularly in cloud infrastructures [37, 38]. Concurrency bugs are statistically different from non-concurrency bugs in terms of required \textbf{fixing time} and \textbf{severity} [38-40]. Fixing Deadlock bugs and Data Race bugs has shown a "large" effect size difference in fixing time [41].

    \item \textbf{Formal Concurrency Analysis}:
    Dedicated tools like **ConcBugAssist** utilize constraint solving and bounded model checking to diagnose failures in multithreaded C programs. This process localizes the \textbf{minimal subset of inter-thread ordering constraints} responsible for an assertion failure, which greatly aids comprehension compared to analyzing full execution traces [34, 42-44]. Tools designed for specific concurrency models, such as GCatch, detect **BMOC** (Blocking Multiple Operations on Channel) bugs in Go by building dependency graphs and using Z3 constraints to ensure no perpetual blocking occurs [45-47].

    \item \textbf{LLM Performance in Concurrency Verification}:
    The ability of LLMs to verify concurrent programs must be assessed under specific memory models, including the standard Sequential Consistency (SC) and more challenging \textbf{Relaxed Memory Models (RMM)} [48]. Evaluation involves targeted questions (Q1-Q5) covering program comprehension, concurrency issue identification (data race, deadlock), and verification under both SC and RMM [48].
\end{enumerate}