\section{Framework: The Design Space of LLM-based APR and Critical Evaluation}
\label{sec:framework}

This section outlines the conceptual framework and classification systems used to analyze the field of Automated Program Repair (APR), particularly focusing on Large Language Model (LLM) driven paradigms (\emph{LLM-APR}) [1]. This framework serves a dual purpose: first, classifying the latest LLM-APR techniques based on their design choices, and second, emphasizing the critical need for a \textbf{comprehensive evaluation} of APR systems that extends beyond simple fix rates to detect harmful side effects [2, 3].

\subsection{Taxonomy of LLM-based APR Methodologies}
\label{ssec:taxonomy}

Code-focused LLMs, such as OpenAI's Codex, CodeLlama, and DeepSeek-Coder, have demonstrated the ability to understand and generate code, facilitating bug fixing with minimal human intervention [1]. The current landscape of LLM-APR, spanning January 2022 to June 2025, covers 63 distinct systems [4]. These methods are classified based on the dimensions of model \emph{parameters} (whether they are updated) and \emph{control flow} (who dictates the subsequent step) [4].

\begin{enumerate}
    \item \textbf{Fine-Tuning (FT) Approaches}:
    These methods update the weights of pre-trained LLMs using datasets of bug-fix pairs, resulting in strong task-specific adaptation and higher accuracy [5, 6]. Variants include Full Fine-Tuning (e.g., VulMaster, RepairCAT) [4, 7], Parameter-Efficient Fine-Tuning (PEFT, e.g., RepairLLaMA, MORepair) [4, 7, 8], Knowledge Distilling (e.g., KNOD, DistiLRR) [4, 9, 10], and Reinforcement Learning Fine-Tuning (RLFT) [4, 10]. A drawback of full FT is its heavy GPU demand and frequent overfitting to the suspected bug location [6].

    \item \textbf{Prompting Approaches}:
    The pre-trained model remains \textbf{frozen} and generates a patch in a single turn [11]. Deployment is rapid as it requires no additional training cost [11].
    \begin{itemize}
        \item \emph{Zero-Shot Prompting}: Provides only the buggy code and a terse instruction (e.g., "fix the bug") [11]. Early studies showed that zero-shot LLMs (like Codex) could outperform state-of-the-art APR techniques, but performance is sensitive to subtle prompt edits [11, 12].
        \item \emph{Few-Shot Prompting}: Includes a small set of bug-fix exemplars before the target defect to infer an edit pattern, often stabilizing generation and improving accuracy [13]. The success of this approach relies on careful curation or automated retrieval of highly similar examples [14, 15].
    \end{itemize}

    \item \textbf{Procedural Pipelines}:
    LLM calls are embedded into fixed, multi-step workflows to ensure reproducible output [4]. Examples include iterative testing loops (\emph{Test-in-the-Loop}, e.g., ChatRepair, ThinkRepair) [4, 16] and human intervention protocols (\emph{Human-in-the-Loop}, e.g., CREF) [4].

    \item \textbf{Agentic Frameworks}:
    The LLM operates as an autonomous agent that \textbf{decides the next step} and invokes external tools (such as testing, compilation, or static analysis) [4, 17].
    \begin{itemize}
        \item \emph{Tool-Augmented Agents} (e.g., SWE-Agent, RepairAgent, LANTERN): These agents improve over one-shot prompting by allowing the LLM to choose external actions [17, 18].
        \item \emph{LLM-as-Judges} (e.g., VulSifter, LLM4PatchCorrect): Models are used as critics to rank or refine patch candidates based on various criteria [4, 18, 19].
    \end{itemize}
\end{enumerate}

\subsubsection*{Cross-Cutting Augmentation Layers}
The efficacy of these paradigms is frequently enhanced by augmenting the context provided to the LLM:
\begin{itemize}
    \item \textbf{RAG} (\emph{Retrieval-Augmented Generation}): Incorporates external, high-signal structural context, such as relevant code snippets fetched from a repository using a Knowledge Graph (e.g., KGCompass) or dynamically collected failing assertions (e.g., D4C) [20-22].
    \item \textbf{AAG} (\emph{Analysis-Augmented Generation}): Integrates results from program analysis tools, such as semantic scoping (Appatch) or static analysis logs, to guide the model's output [21].
\end{itemize}

\subsection{A Critical Framework for APR System Evaluation}
\label{ssec:evaluation}

The evolution of APR evaluation has shifted from measuring the plausibility of generated patches to accurately counting \textbf{correct patches}, primarily due to the widespread issue of \textbf{overfitting} to test suites [2, 23, 24]. To rigorously validate LLM-APR tools, a comprehensive evaluation framework must analyze three critical factors beyond simple test passing:

\begin{enumerate}
    \item \textbf{New Violations and Code Smells}:
    It is crucial to assess whether the repair process introduces new coding defects or code smells [25, 26]. The Sorald case study demonstrated that while fixing SonarQube issues, the tool introduced \textbf{2120 new violations} (including 32 bugs and 2088 code smells) [26, 27].

    \item \textbf{Functional Correctness and Semantic Preservation}:
    APR evaluation must verify that the generated patch maintains the original program's intended behavior [28]. This is typically validated by rerunning the original \textbf{unit tests} on the patched code [3].

    \item \textbf{Code Structure Degradation}:
    The impact of the repair on code quality and maintainability must be measured [25]. Metrics from the Chidamber and Kemerer (CK) metrics suite, such as WMC (Weighted Methods per Class), DIT (Depth of Inheritance Tree), CBO (Coupling Between Objects), LCOM1 (Lack of Cohesion of Methods), and LOC (Lines of Code), are used for this analysis [25, 29-31]. Statistical analysis (e.g., Wilcoxon signed rank test) is necessary to determine if metrics significantly change post-repair [25, 29]. For instance, Sorald repairs were found to increase metrics like LCOM1, LOC, and WMC, indicating a degradation of code quality [29].
\end{enumerate}

\subsection{Special Challenge: Concurrency Bug Analysis}
\label{ssec:concurrency}

Concurrency remains a significant challenge for automated tools, including LLMs, due to the inherent \textbf{non-determinism} and the subtle interactions between threads, which lead to an "astronomical number of interleavings" [32-35].

\begin{enumerate}
    \item \textbf{Nature of Concurrency Bugs}:
    \textbf{Data races} (race condition vulnerabilities) are the most prevalent concurrency bugs, accounting for a staggering **80\%** of all such defects [36]. \textbf{Deadlocks} are also critical issues in complex environments, particularly in cloud infrastructures [37, 38]. Concurrency bugs are statistically different from non-concurrency bugs in terms of required \textbf{fixing time} and \textbf{severity} [38-40]. Fixing Deadlock bugs and Data Race bugs has shown a "large" effect size difference in fixing time [41].

    \item \textbf{Formal Concurrency Analysis}:
    Dedicated tools like **ConcBugAssist** utilize constraint solving and bounded model checking to diagnose failures in multithreaded C programs. This process localizes the \textbf{minimal subset of inter-thread ordering constraints} responsible for an assertion failure, which greatly aids comprehension compared to analyzing full execution traces [34, 42-44]. Tools designed for specific concurrency models, such as GCatch, detect **BMOC** (Blocking Multiple Operations on Channel) bugs in Go by building dependency graphs and using Z3 constraints to ensure no perpetual blocking occurs [45-47].

    \item \textbf{LLM Performance in Concurrency Verification}:
    The ability of LLMs to verify concurrent programs must be assessed under specific memory models, including the standard Sequential Consistency (SC) and more challenging \textbf{Relaxed Memory Models (RMM)} [48]. Evaluation involves targeted questions (Q1-Q5) covering program comprehension, concurrency issue identification (data race, deadlock), and verification under both SC and RMM [48].
\end{enumerate}

\section{Classification and Trends in AI-Based APR}

Automated Program Repair (APR) methodologies are systematically classified based on a dual perspective, reflecting the evolution and current state of the art [24].

\subsection{Categorization of APR Approaches}
APR methods are categorized by their underlying \textbf{Learning Paradigm} and their \textbf{Fundamental Approach Type} [24].

\begin{enumerate}
    \item \textbf{Learning Paradigms}: The literature shows an equal representation of \textbf{Supervised} (46.3\%) and \textbf{Unsupervised} (46.3\%) learning approaches [25]. \textbf{Reinforcement Learning} (RL) has seen limited exploration, accounting for only 7.4\% [25].
    \item \textbf{Fundamental Approach Types}: While traditional \textbf{Template-Based} methods constituted a significant portion (29.3\%), there is a clear evolution toward \textbf{Deep Learning (DL)} solutions (43.9\%) in recent years [25]. This shift indicates the growing reliance on advanced AI models for tackling repair tasks [17].
\end{enumerate}
The dominant target programming language in APR research remains Java (68.3\%), suggesting potential for expansion into other languages [25].

\section{Deep Learning and Large Language Models (LLMs) in Repair}

Deep Learning (DL) methods represent the current frontier in APR, aiming to enhance software reliability and maintainability by automatically generating patches [166].

\subsection{Core DL Architectures and Tools}
\begin{enumerate}
    \item \textbf{T5APR (Text-To-Text Automated Program Repair)}: This is a novel neural program repair approach that provides a unified solution for bug fixing across multiple programming languages, including Java, Python, C, and JavaScript [162, 166]. T5APR leverages \textbf{CodeT5}, a powerful pre-trained text-to-text transformer model [163, 166]. It employs a \textbf{checkpoint ensemble strategy} during inference to improve patch recommendations [163, 166] and works by generating candidate patches based on learned contextual information, which are then validated by test cases [164].
    \item \textbf{DeepFix}: This tool addresses common C programming errors using a multilayered sequence-to-sequence neural network with attention, often trained on student-submitted erroneous programs. DeepFix can achieve significant success in automated bug fixing without external error localization tools, relying on compiler-verified iterative corrections [32, 49].
    \item \textbf{CodeBERT}: This is a bimodal pretrained model built on a transformer architecture, utilizing a hybrid objective function that combines masked language modeling with token detection to understand both programming and natural languages [32].
\end{enumerate}

\subsection{Effectiveness by Bug Type}
Deep Learning techniques have proven highly effective for certain bug categories. For instance, \textbf{Syntax errors} exhibit the highest success rate (90â€“95\%) among all bug types addressed by APR. This success is often attributed to specialized tools like \emph{SynFix} (which uses LSTM networks) and \emph{Sensibility} (which uses n-gram models) [41].

\section{Advanced Paradigms and Optimization}

Current research directions focus on improving the accuracy, efficiency, and context-awareness of AI-based APR systems [19].

\subsection{Context-Aware APR}
A crucial approach involves developing context-aware APR systems that improve repair accuracy by utilizing diverse information such as bug reports, code surroundings, and semantic relationships [19, 49].
\begin{itemize}
    \item \textbf{iFixR}: Automates repairs by mining and clustering fix patterns from active project repositories, leveraging bug-fixing commits and prioritizing patches that match successful historical repairs [31, 49].
    \item \textbf{CURE}: Employs a fine-tuned transformer model to capture the rich context of code snippets, utilizing the attention mechanism to capture long-range dependencies and semantic relationships for semantically coherent repairs [49].
    \item \textbf{DeepFix} and \textbf{SimFix}: DeepFix captures contextual information by processing code segments sequentially using an encoder-decoder architecture, while SimFix combines existing patches and similar code snippets based on the error context [49].
\end{itemize}

\subsection{Hybrid and Resource-Efficient Approaches}
The goal of resource efficiency (RQ3) aims to make DL-based APR more accessible for practical applications through techniques like transfer learning, model compression, and hybrid analysis [19, 47, 48].

\begin{itemize}
    \item \textbf{Hybrid Approaches}: Systems like \textbf{RewardRepair} adopt a Neural Machine Translation (NMT) approach, integrating both syntactic (code context-aware learning) and semantic (compilation and test execution feedback) information to ensure patches are syntactically correct and semantically valid [45, 47].
    \item \textbf{LLM Optimization}: Leveraging large language models (LLMs) has led to resource-efficient tools, such as \textbf{PyDex} (which uses multi-modal prompts and iterative querying, achieving a high repair rate of 96.5\% using few-shot learning) and \textbf{RepairCAT} (which fine-tunes the StarCoder-1B model) [47].
    \item \textbf{Optimization Strategies}: Techniques such as iterative repair paradigms (like \textbf{ITER}), transfer learning for specialization (like \textbf{VRepair} for vulnerability repair), and two-stage approaches (like \textbf{CodeReviser}) are used to optimize resource usage and computational overhead [47]. Transfer learning and model compression are essential for developing effective, scalable, and efficient DL-based APR solutions [48].
\end{itemize}